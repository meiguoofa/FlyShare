---
layout:     post
title:      论文学习笔记系列3
subtitle:   UGATIT论文解读
date:       2020-08-25
author:     flyshare
header-img: img/post-bg-hacker.jpg
catalog: true
tags:
    - thesis
---


### UGATIT 的进步与特点

<p align='center'>
      <img src="/img/UGATIT-1.png">
</p>

* 我们提出了一个新奇的带有**注意力模块**和新的归一化函数**AdaLIN**的无监督图像迁移的方法
* 通过区分由辅助分类器分别在源域和目标域获得的注意力图，我们的注意力模块帮助模型知道那里需要密集转换
*AdaLIN归一化帮助注意力引导的模型在不修改模型的结构和超参数的情况下，灵活的控制形状和质地上数量的改变


### UGATIT Generator

```python

def forward(self, inputs):
        print('inputs: ',inputs.shape)
        x = self.down_layer3(inputs)
        print('layer3:',x.shape)
        x = self.down_layer1(x)
        print('layer1:',x.shape)
        x = self.down_layer2(x)
        print('layer2:',x.shape)
        #截至到这一步上采样结束了，获得了上采样的编码
        gap = torch.nn.functional.adaptive_avg_pool2d(x, 1)
        print('gap',gap.shape)
        #全局平均池化
        #print('gap1',gap.shape)
        gap_logit = self.gap_fc(gap.view(x.shape[0], -1))
        gap_weight = list(self.gap_fc.parameters())[0]
        #print('gap_weight',gap_weight)
        print('gap_weight',gap_weight.shape)
        gap = x * gap_weight.unsqueeze(2).unsqueeze(3)
        print('gap2',gap.shape)
        gmp = torch.nn.functional.adaptive_max_pool2d(x, 1)
        gmp_logit = self.gmp_fc(gmp.view(x.shape[0], -1))
        gmp_weight = list(self.gmp_fc.parameters())[0]
        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)

        cam_logit = torch.cat([gap_logit, gmp_logit], 1)
        x = torch.cat([gap, gmp], 1)
        print('合并后的shape',x.shape)
        x = self.relu(self.conv1x1(x))

        x_ = self.fc(x.view(x.shape[0], -1))
        gamma, beta = self.gamma(x_), self.beta(x_)

        for i in range(4):
            x = getattr(self, "ResNetAdaILNBlock_" + str(i + 1))(x, gamma, beta)
        out = self.up_layer(x)

        return out, cam_logit

```

<p align='center'>
      <img src="/img/UGATIT-3.png">
</p>


**生成器前向传播**
* 首先经过down-sample 由[b,3,96,96] -> [b,256,24,24] Encoder feature map
* 全局平均池化(global average pooling) 和 全局最大池化得到 [B,256,1,1] 再经过全卷积层得到logit[B,1] 分别将两个池化的logit concat
* 获得两个全连接层的weight权重作为gap_weight[256.1],gmp_weight[256,1],将这两个weight展开后分别和Encoder feature map Element-Wise得到Attention Feature Map [b,512,24,24] concat后用1*1 conv [b,256,24,24]
* ResNetAdaILNBlock上采样恢复原来图片大小



**CAM & Auxiliary classifier**

* 通过Encoder编码阶段得到特征图，然后对特征图分别进行全局最大池化和全局平均池化，经过全连接层得到一个节点的预测，然后将这个全连接层的参数和特征图相乘得到attention的特征图


<p align='center'>
      <img src="/img/UGATIT-4.png">
</p>

**AdaLIN**

`结合Layer Normalization 和 Instance Normalization的特点`

<p align='center'>
      <img src="/img/UGATIT-2.png">
</p>

<p align='center'>
      <img src="/img/UGATIT-5.png">
</p>


`引用自其他博客，我并不是太懂这个`
* AdaIN的前提是保证通道之间不相关，因为它仅对图像map本身做归一化，文中说明AdaIN会保留稍多的内容结构，而LN则并没有假设通道相关性，它做了全局的归一化，却不能很好的保留内容结构，AdaLIN的设计正是为了结合AdaIN和LN的优点。

### UGATIT Discriminator

* 判别器由全局判别器和局部判别器组成，全局判别器的参数layers=5,局部判别器的layers=7,判别器的网络结构和生成器网络结构非常相似，都是经过Encoder后得到Feature Map,再由Feature Map经过平均和最大池化在经过全连接层得到logits,平均和池化两个logits concat,再将两个全连接层的参数取出来和Encoder得到的Feature Map相乘得到Attention Feature Map 


```
#引用自其他博客
虽然在判别器下CAM并没有做域的分类，但是加入注意力模块对于判别图像真伪是有益的，文中给出的解释是注意力图通过关注目标域中的真实图像和伪图像之间的差异来帮助进行微调
```

### 损失函数

* 对抗损失

<p align='center'>
      <img src="/img/UGATIT-6.png">
</p>

* 循环一致性损失，为了解决模式坍塌问题

<p align='center'>
      <img src="/img/UGATIT-7.png">
</p>

* 特征损失，保持原图的颜色

<p align='center'>
      <img src="/img/UGATIT-8.png">
</p>

* CAM损失 ,生成器中是 $$
\mathcal L_{cam} = -(\mathbb E_{x \sim X_s}[log(\eta_s(x))]+\mathbb E_{x \sim X_t}[log(1 - \eta_t(x))])
$$利用了交叉熵损失，在判别器中则是对真假图像的CAM进行了对抗损失优化，主要是为了在注意图上进一步区分真假图像

<p align='center'>
      <img src="/img/UGATIT-9.png">
</p>

* 最后得到的损失函数是

<p align='center'>
      <img src="/img/UGATIT-10.png">
</p>