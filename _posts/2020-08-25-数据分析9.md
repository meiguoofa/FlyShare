---
layout:     post
title:      数据分析9
subtitle:   第三章第二节：建立和评估--评估
date:       2020-08-25
author:     flyshare
header-img: img/post-bg-hacker.jpg
catalog: true
tags:
    - Data Analysis
---




## 第三章 模型搭建和评估-评估

**根据之前的模型的建模，我们知道如何运用sklearn这个库来完成建模，以及我们知道了的数据集的划分等等操作。那么一个模型我们怎么知道它好不好用呢？以至于我们能不能放心的使用模型给我的结果呢？那么今天的学习的评估，就会很有帮助。**


```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import Image
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
plt.rcParams['figure.figsize'] = (10, 6)  # 设置输出图片大小

**任务：加载数据并分割测试集和训练集**

from sklearn.model_selection import train_test_split
train = pd.read_csv('train.csv')
data = pd.read_csv('clear_data.csv')
X = data
y = train['Survived']
X_train,X_test,y_train,y_test = train_test_split(X, y, stratify=y, random_state=0)
lr = LogisticRegression()
lr.fit(X_train,y_train)
```


### 模型评估

* 模型评估是为了知道模型的泛化能力。
* 交叉验证（cross-validation）是一种评估泛化性能的统计学方法，它比单次划分训练集和测试集的方法更加稳定、全面。
* 在交叉验证中，数据被多次划分，并且需要训练多个模型。
* 最常用的交叉验证是 k 折交叉验证（k-fold cross-validation），其中 k 是由用户指定的数字，通常取 5 或 10。
* 准确率（precision）度量的是被预测为正例的样本中有多少是真正的正例
* 召回率（recall）度量的是正类样本中有多少被预测为正类
* f-分数是准确率与召回率的调和平均

**将上面的概念进一步的理解，大家可以做一下总结:**


准确率: 被预测为正例的样本中的真正的正例 / 被预测为正例的样本
召回率: 正类样本中被预测为正例的样本 / 正类样本

#### 任务一：交叉验证

* 用10折交叉验证来评估之前的逻辑回归模型
* 计算交叉验证精度的平均值


#提示：交叉验证

<p align='center'>
      <img src="/img/Snipaste_2020-01-05_16-37-56.png">
</p>

#### 提示4

* 交叉验证在sklearn中的模块为`sklearn.model_selection`

```python
from sklearn.model_selection import cross_val_score

lr = LogisticRegression()
scores = cross_val_score(lr, X_train, y_train, cv=10)
```


# k折交叉验证分数

```python
scores
```
<p align='center'>
      <img src="/img/dataanalysis.png">
</p>

# 平均k折交叉验证分数

```python
scores.mean()
```

<p align='center'>
      <img src="/img/dataanalysis-2.png">
</p>



#### 思考4

**k折越多的情况下会带来什么样的影响？**


计算量太大，耗费时间太长


#### 任务二：混淆矩阵

* 计算二分类问题的混淆矩阵
* 计算精确率、召回率以及f-分数

什么是二分类问题的混淆矩阵，理解这个概念，知道它主要是运算到什么任务中



<p align='center'>
      <img src="/img/Snipaste_2020-01-05_16-38-26.png">
</p>



#提示：准确率 (Accuracy),精确度（Precision）,Recall,f-分数计算方法

<p align='center'>
      <img src="/img/Snipaste_2020-01-05_16-39-27.png">
</p>



#### 提示5

* 混淆矩阵的方法在sklearn中的`sklearn.metrics`模块
* 混淆矩阵需要输入真实标签和预测标签
* 精确率、召回率以及f-分数可使用`classification_report`模块

```python
from sklearn.metrics import confusion_matrix

lr = LogisticRegression()
lr.fit(X_train,y_train)

pred = lr.predict(X_train)

confusion_matrix(y_train,pred)

from sklearn.metrics import classification_report
print(classification_report(y_train,pred))
```

<p align='center'>
      <img src="/img/dataanalysis-3.png">
</p>



**如果自己实现混淆矩阵的时候该注意什么问题**


* 矩阵，可以理解为就是一张表格，混淆矩阵其实就是一张表格而已

* 以分类模型中最简单的二分类为例，对于这种问题，我们的模型最终需要判断样本的结果是0还是1，或者说是positive还是negative

* True Positive: 真 模型也判为真
* True Negative: 假 模型也判为假
* False Negative 真 模型判为假
* False Positive: 假 模型判为真 

* Accuracy: (TP+TF)/(TP+TF+NF+NP)
* Precesion: TP/(TP+NP) 预测为真实样本中真实样本比例
* Recall: TP/(TP+NF) 真实样本中预测为真实样本比例

#### 任务三：ROC曲线

**绘制ROC曲线**

**什么是ROC曲线，ROC曲线的存在是为了解决什么问题？**

* ROC:接收者操作特征曲线（receiver operating characteristic curve，或者叫ROC曲线）
* 是一种坐标图式的分析工具
* 曲线下面积（AUC）越大通常越好
* 分类模型（又称分类器，或诊断）是将一个实例映射到一个特定类的过程。ROC分析的是二元分类模型，也就是输出结果只有两种类别的模型
* 当讯号侦测（或变数测量）的结果是一个连续值时，类与类的边界必须用一个阈值（英语：threshold）来界定。举例来说，用血压值来检测一个人是否有高血压，测出的血压值是连续的实数（从0~200都有可能），以收缩压140／舒张压90为阈值，阈值以上便诊断为有高血压，阈值未满者诊断为无高血压。

#### 提示6
* ROC曲线在sklearn中的模块为`sklearn.metrics`
* ROC曲线下面所包围的面积越大越好

```python
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test ,lr.decision_function(X_test))  # 从score( lr.decision_function(X_test)) 中获取 1/3 的点作为阈值
#print(len(fpr))
#print(len(tpr))
#print(X_test.shape)
#print(len(lr.decision_function(X_test)))
#print(sorted(lr.decision_function(X_test)))
#print(sorted(thresholds))
#print(len(thresholds))
# tpr = TP / (TP + FN)
# fpr = FP / (TN + FP)
plt.plot(fpr ,tpr , label='ROC Curve')
plt.xlabel("FPR")
plt.ylabel("TPR (recall)")
close_zero = np.argmin(np.abs(thresholds))
plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10, label="threshold zero", fillstyle="none", c='k', mew=2)
plt.legend(loc=4)
```
<p align='center'>
      <img src="/img/dataanalysis-4.png">
</p>


#### 思考6
**对于多分类问题如何绘制ROC曲线**


* 根据Score得到阈值(1/3 Score的数量,阈值按照从大到小从Score中获取)->依次按照阈值给Score编码->得到的编码(pred_label)与true_label进行计算得到混淆矩阵->根据混淆矩阵后计算tpr(真阳性)/fpr(假阳性)

**你能从这条ROC曲线的到什么信息？这些信息可以做什么？**

```
ROC曲线下方的面积（英语：Area under the Curve of ROC (AUC ROC)），其意义是：

因为是在1x1的方格里求面积，AUC必在0~1之间。
假设阈值以上是阳性，以下是阴性；
若随机抽取一个阳性样本和一个阴性样本，分类器正确判断阳性样本的值高于阴性样本之几率 
简单说：AUC值越大的分类器，正确率越高。
从AUC判断分类器（预测模型）优劣的标准：

AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测
```