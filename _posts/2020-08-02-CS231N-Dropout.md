---
layout:     post
title:      CS231N
subtitle:   DropOut
date:       2020-07-30
author:     flyshare
header-img: img/tag-bg-o.jpg
catalog: true
tags:
    - cs231n
---

```python  
  
  #Dropout 前向传播

def dropout_forward(x, dropout_param):
    """
    Performs the forward pass for (inverted) dropout.

    Inputs:
    - x: Input data, of any shape
    - dropout_param: A dictionary with the following keys:
      - p: Dropout parameter. We drop each neuron output with probability p(概率p)，是对输入的值使用，而不是对权重使用.
      - mode: 'test' or 'train'. If the mode is train, then perform dropout;
        if the mode is test, then just return the input.
      - seed: Seed for the random number generator. Passing seed makes this
        function deterministic, which is needed for gradient checking but not
        in real networks.

    Outputs:
    - out: Array of the same shape as x.
    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout
      mask that was used to multiply the input; in test mode, mask is None.
    """
    p, mode = dropout_param['p'], dropout_param['mode']
    if 'seed' in dropout_param:
        np.random.seed(dropout_param['seed'])

    mask = None
    out = None

    if mode == 'train':
        mask = (np.random.rand(*x.shape) > p) / (1 - p)  #以概率p断开神经元部分连接，获得对应的mask掩码
        print(mask)
        out = x * mask

    elif mode == 'test':

        out = x


    cache = (dropout_param, mask)
    out = out.astype(x.dtype, copy=False)

    return out, cache

```



```python

  # dropout的反向传播还挺简单的

  def dropout_backward(dout, cache):
    """
    Perform the backward pass for (inverted) dropout.

    Inputs:
    - dout: Upstream derivatives, of any shape
    - cache: (dropout_param, mask) from dropout_forward.
    """
    dropout_param, mask = cache
    mode = dropout_param['mode']

    dx = None
    if mode == 'train':

        dx = dout * mask
 
    elif mode == 'test':
        dx = dout
    return dx


```