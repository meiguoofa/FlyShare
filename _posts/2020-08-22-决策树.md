---
layout:     post
title:      机器学习算法
subtitle:   决策树
date:       2020-08-22
author:     flyshare
header-img: img/post-bg-hacker.jpg
catalog: true
tags:
    - Algorithm
---

# 决策树

**分类与回归：** 分类目标属性y是离散的，回归目标属性y是连续的

**决策树的基本组成部分：**  决策结点、分支和叶子

```

决策树中最上面的结点称为根结点。是整个决策树的开始。每个分支是一个新的决策结点，或者是树的叶子。每个决策结点代表一个问题或者决策.通常对应待分类对象的属性。每个叶结点代表一种可能的分类结果

在沿着决策树从上到下的遍历过程中，在每个结点都有一个测试。对每个结点上问题的不同测试输出导致不同的分枝，最后会达到一个叶子结点。这一过程就是利用决策树进行分类的过程，利用若干个变量来判断属性的类别

```

<p align='center'>
      <img src="/img/DT-1.png">
</p>

### 决策树与条件概率分布

-决策树表示给定**特征条件**下类的**条件概率**分布。
-条件概率分布定义在特征空间的一个**划分**(partition)上.将特征空间划分为互不相交的**单元**(cell)或**区域**(region),并在每个单元定义一个类的概率分布就构成了一个条件概率分布。
-决策树的一条路径对应于划分中的一个单元。
-决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成

<p align='center'>
      <img src="/img/DT-2.png">
</p>

### 决策树与CLS算法

#### CLS算法

**CLS算法是早期的决策树学习算法。它是许多决策树学习算法的基础**

<p align='center'>
      <img src="/img/DT-3.png">
</p>

**决策树的构建**

<p align='center'>
      <img src="/img/DT-4.png">
</p>

<p align='center'>
      <img src="/img/DT-5.png">
</p>

<p align='center'>
      <img src="/img/DT-6.png">
</p>

#####  CLS算法步骤

-生成一颗空决策树和一张训练样本属性集;
-若训练样本集T 中所有的样本都属于同一类,则生成结点T , 并终止学习算法;否则
-根据某种策略从训练样本属性表中选择属性A 作为测试属性,  生成测试结点A 
-若A的取值为v1,v2,…,vm, 则根据A 的取值的不同,将T 划分成 m个子集T1,T2,…,Tm;
-从训练样本属性表中删除属性A;
-转步骤2, 对每个子集递归调用CLS;


#### ID3算法

**ID3方法使用信息增益度选择测试属性**
**从直觉上讲，小概率事件比大概率事件包含的信息量大。如果某件事情是“百年一见”则肯定比“习以为常”的事件包含的信息量大。
**

##### 信息增益

-熵(entropy)：信息量大小的度量，即表示随机变量不确定性的度量。
-熵的通俗解释：事件ai的信息量I( ai )可如下度量：

<p align='center'>
      <img src="/img/DT-7.png">
</p>

-其中p(ai)表示事件ai发生的概率。
-假设有n个互不相容的事件a1,a2,a3,….,an,它们中有且仅有一个发生，则其平均的信息量(熵)可如下度量：

<p align='center'>
      <img src="/img/DT-8.png">
</p>

- 设X是一个取有限个值的离散随机变量，其概率分布为：

<p align='center'>
      <img src="/img/DT-9.png">
</p>

-则随机变量X的熵定义为

<p align='center'>
      <img src="/img/DT-10.png">
</p>

对数以2为底或以e为底(自然对数)，这时熵的单位分别称作比特(bit)或纳特(nat),熵只依赖于X的分布，与X的取值无关。

<p align='center'>
      <img src="/img/DT-11.png">
</p>

-熵越大，随机变量的不确定性越大：

<p align='center'>
      <img src="/img/DT-12.png">
</p>

-当X为1,0分布时：

<p align='center'>
      <img src="/img/DT-13.png">
</p>
-经验熵：
<p align='center'>
      <img src="/img/DT-15.PNG">
</p>
<p align='center'>
      <img src="/img/DT-14.png">
</p>

-经验条件熵H(Y|X)：表示在己知随机变量X的条件下随机变量Y的不确定性，定义为**X给定条件下**Y的条件概率分布的熵对X的数学期望：

<p align='center'>
      <img src="/img/DT-16.png">
</p>

-当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为**经验熵**（empirical entropy)和**经验条件熵**（empirical conditional entropy )

-信息增益:特征A对训练数据集D的信息增益，g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即 g(D,A)=H(D)-H(D|A)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度.


##### 信息增益的算法

-设训练数据集为D
-|D|表示其样本容量，即样本个数
-设有K个类Ck, k = 1,2，…K，
-|Ck |为属于类Ck的样本个数
-特征A有n个不同的 取值{a1,a2…an}根据特征A的取值将D划分为n个子集D1.。。Dn
-|Di|为 Di的样本个数
-记子集Di中属于类Ck的样本集合为Dik
-|Dik|为Dik的样本个数

-输入：训练数据集D和特征A；
-输出：特征A对训练数据集D的信息增益g(D,A)
1、计算数据集D的经验熵H(D)

<p align='center'>
      <img src="/img/DT-17.png">
</p>

2、计算特征A对数据集D的经验条件熵H(D|A)

<p align='center'>
      <img src="/img/DT-18.png">
</p>

3、计算信息增益

<p align='center'>
      <img src="/img/DT-19.png">
</p>

-以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题

-使用信息增益比可以对这一问题进行校正

-定义5.3（信息增益比） 特征A对训练数据集D的信息增益比定义为信息增益与训练数据集D关于特征A的值的熵之比

<p align='center'>
      <img src="/img/DT-20.png">
</p>

<p align='center'>
      <img src="/img/DT-21.png">
</p>


##### 决策树ID3算法

1. 决定分类属性；
2. 对目前的数据表，建立一个节点N
3. 如果数据库中的数据都属于同一个类，N就是树叶，在树叶上标出所属的类
4. 如果数据表中没有其他属性可以考虑，则N也是树叶，按照少数服从多数的原则在树叶上标出所属类别
5. 否则，根据平均信息期望值E或GAIN值选出一个最佳属性作为节点N的测试属性
6. 节点属性选定后，对于该属性中的每个值：
从N生成一个分支，并将数据表中与该分支有关的数据收集形成分支节点的数据表，在表中删除节点属性那一栏如果分支数据表非空，则运用以上算法从该节点建立子树。

##### 决策树面临的问题

###### 过度拟合
决策树算法增长树的每一个分支的深度，直到恰好能对训练样例比较完美地分类。实际应用中，当数据中有噪声或训练样例的数量太少以至于不能产生目标函数的有代表性的采样时，该策略可能会遇到困难。

